{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Database\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citi Bike provides a valuable tranportation service to people who live in or travel to NYC. It has an [open Citi Bike database](https://ride.citibikenyc.com/system-data) and it's free for the public to [download](https://s3.amazonaws.com/tripdata/index.html). In this notebook, we will build a SQL database populated with the data. Since the data format was changed in Feburary 2021, we'll focus on the NYC's trip data before 2021.\n",
    "  \n",
    "Firstly, we will build up a webscraper to pull all the necessary data. After downloading the data, we will merge files and import the data into a PostgreSQL database. \n",
    "  \n",
    "⚠️A new data format of Citi Bike trip data has been used from February 2021.  \n",
    "⚠️If you encounter issues when you run the codes. Please go and check [Issues](https://github.com/Fitzmon/CitiBike_Analysis/issues?q=is%3Aissue+is%3Aclosed) page.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Refenrence: \n",
    "- [Building a Citibike Database with Python](https://medium.com/@fausto.manon/building-a-citibike-database-with-python-9849a59fb90c)  \n",
    "- [Analysis and prediction of Citi Bike usage in the unpredictable 2020](https://towardsdatascience.com/analysis-and-prediction-of-citi-bike-usage-in-the-unpredictable-2020-3401da26881b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [01. Importing Libraries](#01.-Importing-Libraries)\n",
    "- [02. Data Preparation](#02.-Data-Preparation)\n",
    "- [03. Building a PostgreSQL Database](#03.-Building-a-PostgreSQL-Database)\n",
    "- [04. Unresolved Isuues](#04.-Unresolved-Issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 01. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping libraries\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Downloading, moving and unzipping files\n",
    "import webbrowser\n",
    "from time import sleep\n",
    "import shutil \n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# DataFrame exploration and manipulation\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# PostgreSQL interaction\n",
    "import psycopg2\n",
    "import linecache\n",
    "from psycopg2 import sql\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 02. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the project directory, we run `jupyter.exe notebook` to activate Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s3.amazonaws.com/tripdata/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'xml')\n",
    "data_files = soup.find_all('Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run Jupyter Notebook under the project directory, you can run `os.getcwd()` to get current filepath. Otherwise, you have to input the filepath of folder where you store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current work directory\n",
    "cwd = os.getcwd()\n",
    "data_loc = cwd + '\\data\\\\'\n",
    "#data_loc = '..//data/'\n",
    "\n",
    "# Create a list with picked years\n",
    "years = []\n",
    "for year in range(2013, 2021):\n",
    "    years.append(str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the step below, we're going to retreive all of the zip files from the [site](https://s3.amazonaws.com/tripdata/index.html). Our study area is NYC. As Citi Bike also extends to Jersey City and Hoboken in New Jersey, we will filter the data because we only want the data before 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate empty list\n",
    "zip_files = []\n",
    "filter_files = []\n",
    "\n",
    "# Populate list with zip file names\n",
    "for file in range(len(data_files)-1):\n",
    "    zip_files.append(data_files[file].get_text())\n",
    "\n",
    "# Filter list with data file names\n",
    "for year in years:\n",
    "    for file in zip_files:\n",
    "        if year in file and file not in filter_files: \n",
    "            if 'JC' not in file:\n",
    "                filter_files.append(file)\n",
    "\n",
    "# Sort the list\n",
    "filter_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `webbrowser.open_new(url)` function will open url using the default browser.  \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here I suggest that you <b>change the dafult download location</b> (usually \"C:\\Users\\your_name\\Downloads\") to the project folder from the Settings page in the browser.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download New York City zip files\n",
    "for file in filter_files:\n",
    "    if not os.path.exists(data_loc + file):\n",
    "        webbrowser.open_new(url + file)\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading all of respective .zip files, we will unzip them and then relocate them from the default download folder to the data folder.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'C:/Users/Hui/Downloads/'\n",
    "destination = data_loc\n",
    "#destination = '..//data/'\n",
    "\n",
    "#-----OPTION 1: Save in project folder-----\n",
    "# Unzip files and clean up data folder\n",
    "for file in filter_files:\n",
    "    file_name = destination + file\n",
    "    with ZipFile(file_name) as zip_ref:\n",
    "        zip_ref.extractall(destination)\n",
    "    os.remove(file_name)\n",
    "\n",
    "#-----OPTION 2: Save in default folder-----\n",
    "# Unzip files and clean up data folder\n",
    "# for file in filter_files:\n",
    "#     file_name = source + file\n",
    "#     with ZipFile(file_name) as zip_ref:\n",
    "#         zip_ref.extractall(project_loc)\n",
    "#     os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to uniform the data to match the same data format. In `pandas`, a column in a DataFrame can only have one data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(data_loc)\n",
    "\n",
    "for csv in files:\n",
    "    if csv.endswith('.csv'):\n",
    "        df = pd.read_csv(data_loc + csv)\n",
    "        df = df.rename(columns=({'trip_duration':'tripduration',\n",
    "                             'start_time':'starttime',\n",
    "                             'stop_time':'stoptime',\n",
    "                             'start_station_id':'start station id',\n",
    "                             'start_station_name':'start station name',\n",
    "                             'start_station_latitude':'start station latitude',\n",
    "                             'start_station_longitude':'start station longitude',\n",
    "                             'end_station_id':'end station id',\n",
    "                             'end_station_name':'end station name',\n",
    "                             'end_station_latitude':'end station latitude',\n",
    "                             'end_station _longitude':'end station longitude',\n",
    "                             'bike_id':'bikeid',\n",
    "                             'user_type':'usertype',\n",
    "                             'birth_year':'birth year',\n",
    "                             'gender':'gender'}))\n",
    "        df.to_csv(data_loc + csv, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have formatted all column headers across all downloaded data, we will now separate the data by year and categorise them into different folders named by year from 2013 to 2020. Then, we will merge all `.csv` files into one for each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new folders\n",
    "for year in years:\n",
    "    year_dir = os.path.join(data_loc, year)\n",
    "    if not os.path.exists(year_dir):\n",
    "        os.mkdir(year_dir)\n",
    "    \n",
    "# Move from project folder to folders named by year\n",
    "for item in os.listdir(data_loc):\n",
    "    for year in years:\n",
    "        if year in item and item.endswith('.csv'):\n",
    "            shutil.move(data_loc + item, data_loc + year)\n",
    "\n",
    "# Merge files\n",
    "for year in years:\n",
    "    ny_files = sorted(glob(data_loc + year + '\\*.csv'))\n",
    "    ny_trip_data = pd.concat((pd.read_csv(file) for file in ny_files), ignore_index = True)\n",
    "    ny_trip_data.to_csv(data_loc + year + '/ny_trip_data_' + year + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 03. Building a PostgreSQL Database\n",
    "Please establish a connection with PostgreSQL and create a new database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully in PostgreSQL\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(\"user=postgres password='password'\");\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT);\n",
    "\n",
    "# Obtain in a DB Cursor\n",
    "cursor = conn.cursor();\n",
    "db_name = \"citibike_data\"\n",
    "\n",
    "# Create DB in PostgreSQL\n",
    "create_database = f\"CREATE DATABASE {db_name};\"\n",
    "cursor.execute(create_database);\n",
    "print(\"Database created successfully in PostgreSQL\")\n",
    "\n",
    "# Closing PostgreSQL connection\n",
    "if(conn):\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully in PostgreSQL\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Building a table for New York City data\n",
    "try:\n",
    "    conn = psycopg2.connect(user = 'postgres',\n",
    "                           password = 'password',\n",
    "                           host = '127.0.0.1',\n",
    "                           port = '5432',\n",
    "                           database = 'citibike_data')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Some data types have been amended below to account for all data in the csv (i.e. blank cells)\n",
    "    create_table_query = '''CREATE TABLE new_york_city(\n",
    "                            trip_duration INT,\n",
    "                            start_time TIMESTAMP,\n",
    "                            stop_time TIMESTAMP,\n",
    "                            start_station_id INT,\n",
    "                            start_station_name TEXT,\n",
    "                            start_station_latitude FLOAT,\n",
    "                            start_station_longitude FLOAT,\n",
    "                            end_station_id INT,\n",
    "                            end_station_name TEXT,\n",
    "                            end_station_latitude FLOAT,\n",
    "                            end_station_longitude FLOAT,\n",
    "                            bike_id INT,\n",
    "                            user_type TEXT,\n",
    "                            birth_year TEXT,\n",
    "                            gender INT\n",
    "                            ); '''\n",
    "    \n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(\"Table created successfully in PostgreSQL\")\n",
    "\n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print(\"Error while creating PostgreSQL table:\", error)\n",
    "finally:\n",
    "    # closing database connection\n",
    "    if (conn):\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while updating PostgreSQL table: extra data after last expected column\n",
      "CONTEXT:  COPY new_york_city, line 1: \"680.0,2017-01-01 00:00:21,2017-01-01 00:11:41,3226.0,W 82 St & Central Park West,40.78275,-73.97137,...\"\n",
      "\n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "# Populating the NYC table\n",
    "try:\n",
    "    conn = psycopg2.connect(user = 'postgres',\n",
    "                           password = 'password',\n",
    "                           host = '127.0.0.1',\n",
    "                           port = '5432',\n",
    "                           database = 'citibike_data')\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for year in years:\n",
    "        if year == '2013' or year == '2016': # meet an syntax issue\n",
    "            continue\n",
    "        filename = data_loc + year + '/ny_trip_data_' + year + '.csv'\n",
    "        with open(filename, 'r') as data:\n",
    "            next(data) # skip the header row\n",
    "            cursor.copy_from(data, 'new_york_city', sep=',')\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Table updated successfully in PostgreSQL \")\n",
    "        \n",
    "except (Exception, psycopg2.DatabaseError) as error:\n",
    "    print (\"Error while updating PostgreSQL table:\", error)\n",
    "    \n",
    "finally:\n",
    "    if(conn):\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"PostgreSQL connection is closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. Unsolved Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ~~separate the data by years~~ \n",
    "- ~~improve pd.read() function~~\n",
    "- PostgreSQL errir -> 'Error while updating PostgreSQL table: extra data after last expected column'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webmap",
   "language": "python",
   "name": "webmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
